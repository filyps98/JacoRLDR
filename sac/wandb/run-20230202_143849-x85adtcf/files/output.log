
cuda:0
Soft Q Network (1,2):  SoftQNetwork(
  (batch_norm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv1): Conv2d(3, 64, kernel_size=(2, 2), stride=(1, 1))
  (batchnorm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pooling1): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))
  (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pooling2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv_gen1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2))
  (conv_gen2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
  (linear1): Linear(in_features=256, out_features=224, bias=True)
  (linear_1): Linear(in_features=6, out_features=64, bias=True)
  (linear_2): Linear(in_features=64, out_features=64, bias=True)
  (linear_3): Linear(in_features=64, out_features=32, bias=True)
  (linear_state_combined): Linear(in_features=256, out_features=512, bias=True)
  (linear_combined_1): Linear(in_features=550, out_features=256, bias=True)
  (linear_combined_2): Linear(in_features=256, out_features=128, bias=True)
  (linear_action_1): Linear(in_features=7, out_features=64, bias=True)
  (linear_action_2): Linear(in_features=64, out_features=38, bias=True)
  (linear_final): Linear(in_features=128, out_features=1, bias=True)
)
Policy Network:  PolicyNetwork(
  (batch_norm): BatchNorm2d(3, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv1): Conv2d(3, 64, kernel_size=(2, 2), stride=(1, 1))
  (batchnorm1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pooling1): MaxPool2d(kernel_size=3, stride=3, padding=0, dilation=1, ceil_mode=False)
  (conv2): Conv2d(64, 128, kernel_size=(2, 2), stride=(1, 1))
  (batchnorm2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (pooling2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (conv_gen1): Conv2d(128, 128, kernel_size=(1, 1), stride=(1, 1))
  (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2))
  (conv_gen2): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
  (linear1): Linear(in_features=256, out_features=480, bias=True)
  (linear_1): Linear(in_features=6, out_features=64, bias=True)
  (linear_2): Linear(in_features=64, out_features=64, bias=True)
  (linear_3): Linear(in_features=64, out_features=32, bias=True)
  (linear_combined_1): Linear(in_features=512, out_features=1024, bias=True)
  (linear_combined_2): Linear(in_features=1024, out_features=512, bias=True)
  (mean_linear): Linear(in_features=512, out_features=7, bias=True)
  (log_std_linear): Linear(in_features=512, out_features=7, bias=True)
)
Target Point:
[ 0.30984337 -0.04216463  0.0326244 ]
Target Point:
[ 0.30984337 -0.04216463  0.0326244 ]
Target Point:
[ 0.30984337 -0.04216463  0.0326244 ]
Target Point:
[ 0.30984337 -0.04216463  0.0326244 ]
Target Point:
[ 0.30984337 -0.04216463  0.0326244 ]
Target Point:
[ 0.30984337 -0.04216463  0.0326244 ]
Episode:  0 | Episode Reward:  2.9725082344711433
Target Point:
[0.39926554 0.04044749 0.03575655]
Target Point:
[0.39926554 0.04044749 0.03575655]
Target Point:
[0.39926554 0.04044749 0.03575655]
Episode:  1 | Episode Reward:  -0.0905824135784642
Target Point:
[ 0.36758327 -0.00390084  0.03555706]
Target Point:
[ 0.36758327 -0.00390084  0.03555706]
Target Point:
[ 0.36758327 -0.00390084  0.03555706]
Target Point:
[ 0.36758327 -0.00390084  0.03555706]
Episode:  2 | Episode Reward:  -0.6018192050384463
Target Point:
[ 0.36295117 -0.03014273  0.04255579]
Target Point:
[ 0.36295117 -0.03014273  0.04255579]
Episode:  3 | Episode Reward:  -0.6604735164314743
Target Point:
[0.34432293 0.04660998 0.04795992]
Target Point:
[0.34432293 0.04660998 0.04795992]
Target Point:
[0.34432293 0.04660998 0.04795992]
Target Point:
[0.34432293 0.04660998 0.04795992]
Target Point:
[0.34432293 0.04660998 0.04795992]
Target Point:
[0.34432293 0.04660998 0.04795992]
Episode:  4 | Episode Reward:  1.2356350910737877
Target Point:
[ 3.25843068e-01 -1.44946814e-04  3.37975778e-02]
Episode:  5 | Episode Reward:  -1
Target Point:
[ 0.39850225 -0.01113656  0.04310406]
Episode:  6 | Episode Reward:  -1
Target Point:
[ 0.3912109  -0.0086897   0.04024886]
Target Point:
[ 0.3912109  -0.0086897   0.04024886]
Episode:  7 | Episode Reward:  -0.25489026573260576
Target Point:
[ 0.31258049 -0.01376598  0.04678442]
Target Point:
[ 0.31258049 -0.01376598  0.04678442]
Episode:  8 | Episode Reward:  0.5576327044575762
Target Point:
[ 0.38849535 -0.04410143  0.05115454]
Target Point:
[ 0.38849535 -0.04410143  0.05115454]
Target Point:
[ 0.38849535 -0.04410143  0.05115454]
Episode:  9 | Episode Reward:  0.2091386108030362
Target Point:
[0.39132431 0.00397337 0.0451486 ]
Target Point:
[0.39132431 0.00397337 0.0451486 ]
Target Point:
[0.39132431 0.00397337 0.0451486 ]
Target Point:
[0.39132431 0.00397337 0.0451486 ]
Episode:  10 | Episode Reward:  1.3455530794184978
Target Point:
[ 0.31768052 -0.01249144  0.04536006]
Target Point:
[ 0.31768052 -0.01249144  0.04536006]
Target Point:
[ 0.31768052 -0.01249144  0.04536006]
Target Point:
[ 0.31768052 -0.01249144  0.04536006]
Target Point:
[ 0.31768052 -0.01249144  0.04536006]
Episode:  11 | Episode Reward:  0.1650201187306919
Target Point:
[0.34081918 0.02557878 0.05421096]
Target Point:
[0.34081918 0.02557878 0.05421096]
Target Point:
[0.34081918 0.02557878 0.05421096]
Target Point:
[0.34081918 0.02557878 0.05421096]
Target Point:
[0.34081918 0.02557878 0.05421096]
Target Point:
[0.34081918 0.02557878 0.05421096]
Episode:  12 | Episode Reward:  1.9386587319541226
Target Point:
[ 0.34039806 -0.00638582  0.03919456]
Target Point:
[ 0.34039806 -0.00638582  0.03919456]
Traceback (most recent call last):
  File "train.py", line 158, in <module>
    _=sac_trainer.update(batch_size, reward_scale=10., auto_entropy=AUTO_ENTROPY, target_entropy=-0.05*action_dim)
  File "/home/filippo/Desktop/Project/Experiments/JacoRLDR/sac/sac_trainer.py", line 51, in update
    state_image, state_hand, action, reward, next_state_image, next_state_hand, done = self.replay_buffer.sample(batch_size)
  File "/home/filippo/Desktop/Project/Experiments/JacoRLDR/sac/replay.py", line 68, in sample
    state_image, state_hand, action, reward, next_state_image, next_state_hand, done = map(np.stack, zip(*batch)) # stack for each element
  File "<__array_function__ internals>", line 6, in stack
  File "/home/filippo/miniconda3/envs/robot/lib/python3.7/site-packages/numpy/core/shape_base.py", line 433, in stack
    return _nx.concatenate(expanded_arrays, axis=axis, out=out)
  File "<__array_function__ internals>", line 6, in concatenate
KeyboardInterrupt