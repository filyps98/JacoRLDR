cuda:0
VGG(
  (features): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (13): ReLU(inplace=True)
    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (15): ReLU(inplace=True)
    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (18): ReLU(inplace=True)
    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (20): ReLU(inplace=True)
    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (22): ReLU(inplace=True)
    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (25): ReLU(inplace=True)
    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (27): ReLU(inplace=True)
    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (29): ReLU(inplace=True)
    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  )
  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))
  (classifier): Sequential(
    (0): Linear(in_features=25088, out_features=4096, bias=True)
    (1): ReLU(inplace=True)
    (2): Dropout(p=0.5, inplace=False)
    (3): Linear(in_features=4096, out_features=4096, bias=True)
    (4): ReLU(inplace=True)
    (5): Dropout(p=0.5, inplace=False)
    (6): Linear(in_features=4096, out_features=1000, bias=True)
  )
)
/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:209: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and will be removed in 0.15, please use 'weights' instead.
  f"The parameter '{pretrained_param}' is deprecated since 0.13 and will be removed in 0.15, "
/opt/conda/lib/python3.7/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.
  warnings.warn(msg)
Soft Q Network (1,2):  SoftQNetwork(
  (batch_norm_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (batch_norm_2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv_gen1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
  (conv_gen2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2))
  (conv_gen3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
  (conv_gen4): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2))
  (pooling1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (linear1): Linear(in_features=4096, out_features=480, bias=True)
  (linear_bn_0_1): BatchNorm1d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (linear_bn_0_2): BatchNorm1d(7, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (linear_bn_0_3): BatchNorm1d(550, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (linear_bn_1): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (linear_bn_2): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (final_linear_bn): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (linear_1): Linear(in_features=10, out_features=64, bias=True)
  (linear_2): Linear(in_features=64, out_features=64, bias=True)
  (linear_3): Linear(in_features=64, out_features=32, bias=True)
  (linear_state_combined): Linear(in_features=512, out_features=512, bias=True)
  (linear_combined_1): Linear(in_features=550, out_features=256, bias=True)
  (linear_combined_2): Linear(in_features=256, out_features=256, bias=True)
  (linear_combined_3): Linear(in_features=256, out_features=128, bias=True)
  (linear_action_1): Linear(in_features=7, out_features=64, bias=True)
  (linear_action_2): Linear(in_features=64, out_features=64, bias=True)
  (linear_action_3): Linear(in_features=64, out_features=38, bias=True)
  (linear_final): Linear(in_features=128, out_features=1, bias=True)
)
Policy Network:  PolicyNetwork(
  (feature_extractor): Sequential(
    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (1): ReLU(inplace=True)
    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (3): ReLU(inplace=True)
    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (6): ReLU(inplace=True)
    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (8): ReLU(inplace=True)
    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))
    (11): ReLU(inplace=True)
  )
  (batch_norm_1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (batch_norm_2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
  (conv_gen1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))
  (conv_gen2): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2))
  (conv_gen3): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1))
  (conv_gen4): Conv2d(512, 1024, kernel_size=(3, 3), stride=(2, 2))
  (pooling1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)
  (linear1): Linear(in_features=4096, out_features=480, bias=True)
  (linear_1): Linear(in_features=10, out_features=64, bias=True)
  (linear_2): Linear(in_features=64, out_features=64, bias=True)
  (linear_3): Linear(in_features=64, out_features=32, bias=True)
  (linear_combined_1): Linear(in_features=512, out_features=1024, bias=True)
  (linear_combined_2): Linear(in_features=1024, out_features=1024, bias=True)
  (linear_combined_3): Linear(in_features=1024, out_features=512, bias=True)
  (mean_linear): Linear(in_features=512, out_features=7, bias=True)
  (log_std_linear): Linear(in_features=512, out_features=7, bias=True)
)
Episode:  0 | Episode Reward:  2.6770230338758783
Episode:  1 | Episode Reward:  0.6393333869809772
Episode:  2 | Episode Reward:  0.6577884536884582
Episode:  3 | Episode Reward:  -0.4397844253707168
Episode:  4 | Episode Reward:  0.9390958685297224
Episode:  5 | Episode Reward:  -0.4341567021986712
Episode:  6 | Episode Reward:  -0.15398949931448747
Episode:  7 | Episode Reward:  0.6503327909499819
Episode:  8 | Episode Reward:  -0.6926166825137577
Episode:  9 | Episode Reward:  2.7734705350554596
Episode:  10 | Episode Reward:  1.1894645029629485
Episode:  11 | Episode Reward:  1.0193055980264423
Episode:  12 | Episode Reward:  1.0114790732862755
Episode:  13 | Episode Reward:  3.241306841905479
Episode:  14 | Episode Reward:  -0.28577172374125004
Episode:  15 | Episode Reward:  0.5862168007899453
Episode:  16 | Episode Reward:  0.7886500501993505
Episode:  17 | Episode Reward:  -0.5218023326405685
Episode:  18 | Episode Reward:  1.6002790858042144
Episode:  19 | Episode Reward:  0.4817698504793634
Episode:  20 | Episode Reward:  1.8777616299501503
Episode:  21 | Episode Reward:  9.191435003209088
Episode:  22 | Episode Reward:  2.059113915920395
Episode:  23 | Episode Reward:  0.9900556070858784
Episode:  24 | Episode Reward:  3.9947695772085483
Episode:  25 | Episode Reward:  1.7068654519937465
Episode:  26 | Episode Reward:  0.3617537436693123
Episode:  27 | Episode Reward:  2.5574943089368007
Episode:  28 | Episode Reward:  -0.4743655006869101
Episode:  29 | Episode Reward:  2.2611167681019215
Episode:  30 | Episode Reward:  1.7752160898060874
Episode:  31 | Episode Reward:  0.9318085777388045
Episode:  32 | Episode Reward:  1.1932363602113145
